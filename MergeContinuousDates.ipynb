{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP212dULSPPR/oeug72uco3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsrikanth24/Python-Interview/blob/main/MergeContinuousDates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zVJ5uUoGhGf",
        "outputId": "d562543c-ea27-452f-823e-9f152e41bffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----------+----------+\n",
            "|id |grp |start_date|end_date  |\n",
            "+---+----+----------+----------+\n",
            "|1  |NULL|2025-01-01|2025-01-10|\n",
            "|1  |0   |2025-01-11|2025-01-15|\n",
            "|2  |NULL|2025-02-01|2025-02-05|\n",
            "|2  |1   |2025-02-10|2025-02-15|\n",
            "+---+----+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, sum as _sum, min as _min, max as _max, date_add\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"MergeContinuousDates\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"2025-01-01\", \"2025-01-10\"),\n",
        "    (1, \"2025-01-11\", \"2025-01-15\"),\n",
        "    (2, \"2025-02-01\", \"2025-02-05\"),\n",
        "    (2, \"2025-02-10\", \"2025-02-15\")\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"id\", \"start_date\", \"end_date\"])\n",
        "\n",
        "# Convert dates to proper date format\n",
        "df = df.withColumn(\"start_date\", col(\"start_date\").cast(\"date\")) \\\n",
        "       .withColumn(\"end_date\", col(\"end_date\").cast(\"date\"))\n",
        "\n",
        "# Define a window partitioned by ID and ordered by start_date\n",
        "window_spec = Window.partitionBy(\"id\").orderBy(\"start_date\")\n",
        "\n",
        "# Use lag to find the previous row's end_date\n",
        "df = df.withColumn(\"prev_end_date\", lag(\"end_date\").over(window_spec))\n",
        "\n",
        "# Identify gaps: If the current start_date is NOT continuous with the previous end_date, mark it as a new group\n",
        "# Use date_add to add 1 day to prev_end_date for comparison\n",
        "df = df.withColumn(\n",
        "    \"is_new_group\",\n",
        "    (col(\"start_date\") > date_add(col(\"prev_end_date\"), 1)).cast(\"int\")\n",
        ")\n",
        "\n",
        "# Use a cumulative sum to create a group identifier for continuous ranges\n",
        "df = df.withColumn(\"grp\", _sum(\"is_new_group\").over(window_spec))\n",
        "\n",
        "# Aggregate by ID and group to find the min(start_date) and max(end_date) for each group\n",
        "result = df.groupBy(\"id\", \"grp\") \\\n",
        "           .agg(_min(\"start_date\").alias(\"start_date\"), _max(\"end_date\").alias(\"end_date\")) \\\n",
        "           .orderBy(\"id\", \"start_date\")\n",
        "\n",
        "# Show the result\n",
        "result.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"CREATE TABLE date_ranges (\n",
        "    id INT,\n",
        "    start_date DATE,\n",
        "    end_date DATE\n",
        ");\n",
        "\n",
        "INSERT INTO date_ranges (id, start_date, end_date) VALUES\n",
        "(1, DATE'2025-01-01', DATE'2025-01-10'),\n",
        "(1, DATE'2025-01-11', DATE'2025-01-15'),\n",
        "(2, DATE'2025-02-01', DATE'2025-02-05'),\n",
        "(2, DATE'2025-02-10', DATE'2025-02-15'); \"\"\"\n",
        "\n",
        "code = \"\"\"\n",
        "WITH step1 AS (\n",
        "    SELECT\n",
        "        id,\n",
        "        start_date,\n",
        "        end_date,\n",
        "        LAG(end_date) OVER (PARTITION BY id ORDER BY start_date) AS prev_end_date\n",
        "    FROM date_ranges\n",
        "),\n",
        "step2 AS (\n",
        "    SELECT\n",
        "        id,\n",
        "        start_date,\n",
        "        end_date,\n",
        "        CASE\n",
        "            WHEN prev_end_date IS NULL THEN 0\n",
        "            WHEN start_date > prev_end_date + 1 THEN 1\n",
        "            ELSE 0\n",
        "        END AS is_new_group\n",
        "    FROM step1\n",
        "),\n",
        "step3 AS (\n",
        "    SELECT\n",
        "        id,\n",
        "        start_date,\n",
        "        end_date,\n",
        "        SUM(is_new_group) OVER (PARTITION BY id ORDER BY start_date ROWS UNBOUNDED PRECEDING) AS grp\n",
        "    FROM step2\n",
        ")\n",
        "SELECT\n",
        "    id,\n",
        "    MIN(start_date) AS start_date,\n",
        "    MAX(end_date) AS end_date\n",
        "FROM step3\n",
        "GROUP BY id, grp\n",
        "ORDER BY id, start_date;\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xtNdTrdWIM5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lag, sum as _sum, min as _min, max as _max, date_add, when, coalesce, lit\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MergeContinuousDates\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"2025-01-01\", \"2025-01-10\"),\n",
        "    (1, \"2025-01-11\", \"2025-01-15\"),\n",
        "    (2, \"2025-02-01\", \"2025-02-05\"),\n",
        "    (2, \"2025-02-10\", \"2025-02-15\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"start_date\", \"end_date\"]) \\\n",
        "          .withColumn(\"start_date\", col(\"start_date\").cast(\"date\")) \\\n",
        "          .withColumn(\"end_date\", col(\"end_date\").cast(\"date\"))\n",
        "\n",
        "window_spec = Window.partitionBy(\"id\").orderBy(\"start_date\")\n",
        "\n",
        "df = df.withColumn(\"prev_end_date\", lag(\"end_date\").over(window_spec))\n",
        "print(\"After lag:\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"is_new_group\",\n",
        "    when(col(\"prev_end_date\").isNull(), 0)\n",
        "    .when(col(\"start_date\") > date_add(col(\"prev_end_date\"), 1), 1)\n",
        "    .otherwise(0)\n",
        ")\n",
        "print(\"After is_new_group:\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "df = df.withColumn(\"grp\", _sum(\"is_new_group\").over(window_spec))\n",
        "df = df.withColumn(\"grp\", coalesce(col(\"grp\"), lit(0)))  # fill NULL grp with 0\n",
        "print(\"After cumulative sum grp:\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "result = df.groupBy(\"id\", \"grp\") \\\n",
        "           .agg(_min(\"start_date\").alias(\"start_date\"), _max(\"end_date\").alias(\"end_date\")) \\\n",
        "           .orderBy(\"id\", \"start_date\")\n",
        "\n",
        "print(\"Final result:\")\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9MfYNMsHcRj",
        "outputId": "81d23d76-ad6a-4a17-b652-792cb60a9989"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After lag:\n",
            "+---+----------+----------+-------------+\n",
            "|id |start_date|end_date  |prev_end_date|\n",
            "+---+----------+----------+-------------+\n",
            "|1  |2025-01-01|2025-01-10|NULL         |\n",
            "|1  |2025-01-11|2025-01-15|2025-01-10   |\n",
            "|2  |2025-02-01|2025-02-05|NULL         |\n",
            "|2  |2025-02-10|2025-02-15|2025-02-05   |\n",
            "+---+----------+----------+-------------+\n",
            "\n",
            "After is_new_group:\n",
            "+---+----------+----------+-------------+------------+\n",
            "|id |start_date|end_date  |prev_end_date|is_new_group|\n",
            "+---+----------+----------+-------------+------------+\n",
            "|1  |2025-01-01|2025-01-10|NULL         |0           |\n",
            "|1  |2025-01-11|2025-01-15|2025-01-10   |0           |\n",
            "|2  |2025-02-01|2025-02-05|NULL         |0           |\n",
            "|2  |2025-02-10|2025-02-15|2025-02-05   |1           |\n",
            "+---+----------+----------+-------------+------------+\n",
            "\n",
            "After cumulative sum grp:\n",
            "+---+----------+----------+-------------+------------+---+\n",
            "|id |start_date|end_date  |prev_end_date|is_new_group|grp|\n",
            "+---+----------+----------+-------------+------------+---+\n",
            "|1  |2025-01-01|2025-01-10|NULL         |0           |0  |\n",
            "|1  |2025-01-11|2025-01-15|2025-01-10   |0           |0  |\n",
            "|2  |2025-02-01|2025-02-05|NULL         |0           |0  |\n",
            "|2  |2025-02-10|2025-02-15|2025-02-05   |1           |1  |\n",
            "+---+----------+----------+-------------+------------+---+\n",
            "\n",
            "Final result:\n",
            "+---+---+----------+----------+\n",
            "|id |grp|start_date|end_date  |\n",
            "+---+---+----------+----------+\n",
            "|1  |0  |2025-01-01|2025-01-15|\n",
            "|2  |0  |2025-02-01|2025-02-05|\n",
            "|2  |1  |2025-02-10|2025-02-15|\n",
            "+---+---+----------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}